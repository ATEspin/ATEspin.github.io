[
  {
    "path": "posts/2022-06-18-collaboration-network/",
    "title": "Co-author Network",
    "description": "Building a network of co-authors in R",
    "author": [
      {
        "name": "Abel Torres Espin",
        "url": {}
      }
    ],
    "date": "2022-06-18",
    "categories": [],
    "contents": "\r\nThis post will show you how you can extract and visualize your\r\nnetwork of co-authors. You can also find the code for this post on\r\nGitHub.\r\nLuckily for us, the hard work needed for this has been already\r\nprogrammed by amazing people out there, so we will be using these\r\npackages:\r\n\r\n\r\nlibrary(easyPubMed) #API connection with pubmed\r\nlibrary(tidyverse) #To facilitate some data wrangling\r\nlibrary(igraph) #To build the graph\r\nlibrary(networkD3) #Interactive networks\r\nlibrary(ggnetwork) #for static networks\r\n\r\n\r\n\r\nExtracting Pubmed\r\ninformation\r\nThe easyPubMed package can be used for… yes, you guessed\r\nright… easily extract information from Pubmed. So, the first thing to do\r\nis to search us and extract the list of authors in our papers from the\r\n‘medline’ format, tagged as ‘AU’.\r\n\r\n\r\nmy_entrez_id <- get_pubmed_ids('Torres-Espin A')\r\nmy_author_txt <- fetch_pubmed_data(my_entrez_id, format = \"medline\")\r\nauthors<-my_author_txt[str_detect(my_author_txt, \"^AU \")]\r\ncoauthors_list<-unique(str_remove_all(authors, '^AU  - '))\r\nhead(coauthors_list)\r\n\r\n\r\n\r\nNote that we use unique because we might have more\r\nthan one paper with the same author. We do this because now we are going\r\nto search the list of co-authors to get the relationship of publication\r\nbetween them. If you only want to get the first level connections of\r\neach co-author with you, then do not need to collapse the\r\ncoauthors_list.\r\nNow let’s create a data frame containing the list of co-authors for\r\neach author on our list of co-authors. We can do that by iterating\r\nthrough the same code we did before and saving the results in a list of\r\ndata frames. We will stack the list of data frames at the end by calling\r\nrbind. If you know some R you probably know that this\r\ncode can be collapsed in fewer lines and be more efficient using\r\napply. I like using loops from time to time!\r\n\r\n\r\nauthor_list<-list()#define the list to save the data frames\r\n\r\nfor (i in 1:length(coauthors_list)){\r\n  my_entrez_id <- get_pubmed_ids(coauthors_list[i])\r\n  my_author_txt <- fetch_pubmed_data(my_entrez_id, format = \"medline\")\r\n  \r\n  if (length(my_author_txt)==0) next()\r\n  \r\n  authors<-my_author_txt[str_detect(my_author_txt, \"^AU \")]\r\n  authors<-str_remove_all(authors, '^AU  - ')\r\n  author_list[[i]]<-data.frame(co_authors=authors, author=coauthors_list[i])\r\n}\r\nauthor_df<-do.call(rbind, author_list)#stack data frames\r\n\r\n\r\n\r\nWe can save this for later use so that we do not need to do the\r\nsearch again!\r\n\r\n\r\nsaveRDS(author_df, \"author_df.Rds\")\r\n\r\n\r\n\r\nThe next step is filtering our data frame to keep only the co-authors\r\nof our co-authors that are also our co-authors (this is getting\r\ntricky…). You could leave the co-authors of your co-authors that you\r\nhave not published with, but that can make the graph quite big and not\r\nuseful for plotting. After filtering we create a new variable with the\r\njoint string between co-author and author, so that we can count how many\r\ntimes that joint happens.\r\n\r\n\r\nauthor_df<-author_df[author_df$co_authors%in%author_df$author,]%>%\r\n  mutate(co_edge=paste(co_authors, author, sep = '_'))%>%\r\n  group_by(co_edge)%>%\r\n  summarise(count=n())%>%\r\n  separate(co_edge, c('co_authors','author'),sep = '_')\r\n\r\n\r\n\r\nWe are ready to create the graph from the data frame. We specify the\r\nweight of the edges (using E) as the count of the times two\r\nauthors published together. Note that in our count data ‘author 1-author\r\n2’ counts different than ‘author 2-author 1’ for the same paper, so we\r\nare counting each article twice. That is not a problem since we are\r\nconsidering a undirected graph. We just need to remember that we will be\r\nrepresenting each article 2 times. I did not care because graphically\r\ndoes not matter. Now, we plot the graph (very rough!)\r\n\r\n\r\nauthor_g<-graph_from_data_frame(author_df,directed = F)\r\nE(author_g)$weight<-author_df$count\r\nauthor_g<-simplify(author_g)\r\nplot(author_g,edge.width = E(author_g)$weight)\r\n\r\n\r\n\r\n\r\nLast step is to make the graph pretty and play with the\r\nparameters.\r\nFor a good network visualization tutorial in R check [this post]( https://mr.schochastics.net/material/netvizr/)\r\nUPDATE\r\nI decided to use the networkD3 package for\r\ncomputational efficiency. In the initial version of this post I used the\r\nvisNetwork package, which I find an amazing tool, so I\r\nrecommend you explore it if you use networks.\r\n\r\n\r\ngroup<-rep(1, length(V(author_g)))\r\ngroup[189]<-2\r\n\r\nauthor_d3 <- igraph_to_networkD3(author_g, group = as.character(group))\r\n\r\nforceNetwork(Links = author_d3$links, Nodes = author_d3$nodes, \r\n             Source = 'source', Target = 'target', NodeID = 'name', \r\n             Group = \"group\", linkColour = \"lightgrey\", \r\n             colourScale =JS('d3.scaleOrdinal().domain([\"1\", \"2\"]).range([\"#ADD8E6\", \"#FF6600\"])'), \r\n             opacity = 0.9)\r\n\r\n\r\n\r\n\r\nCode from initial post\r\n\r\n\r\nvis_g<-toVisNetworkData(author_g) #coverts the igraph object to visNetwork\r\nvis_g$edges$value<-vis_g$edges$weight #specify the values for eadges as the weights (number of papers for pairs)\r\n\r\n#vis_g$nodes$title<-co_authors_abel$author_name #name of the nodes\r\n\r\n#plot the network  \r\nvisNetwork(nodes = vis_g$nodes, edges = vis_g$edges, height = \"400px\", width = '500px')%>%\r\n  visNodes(size = 10, shape='cicle', mass=2,font=list(color='white'),fixed = F) %>%\r\n  visEdges(length = 10,physics = T, smooth = list(enabled=T))%>%\r\n  visLayout(randomSeed = 666)%>%\r\n  visOptions(highlightNearest = list(hover = T), \r\n             nodesIdSelection = list(enabled = TRUE))%>%\r\n  visPhysics(stabilization = T, minVelocity = 10, maxVelocity = 25)\r\n\r\n\r\n\r\nOr we can use the ggnetwork package for static networks. It is much\r\nfaster to plot and integrates with ggolot2!\r\n\r\n\r\nggplot(ggnetwork(author_g), \r\n       aes(x = x, y = y, xend = xend, yend = yend)) +\r\n  geom_edges(aes(size = weight/100), color = \"grey50\", curvature = 0.1, alpha= 0.2, \r\n             show.legend = F) +\r\n  geom_nodes(aes(size = weight+weight*2), show.legend = F, color = \"firebrick1\", alpha = 0.5) +\r\n  geom_nodes(aes(size = weight), show.legend = F, color = \"Orange\") +\r\n  theme_blank()\r\n\r\n\r\n\r\n\r\nDONE!\r\nNow you can save the network as html using the saveNetwork\r\nfunction. For the network in my co-authors section, I did some extra\r\nprocessing steps to consider that some times same author might publish\r\nwith slightly different names. I also added some groups and colors. I\r\nwould let you be creative.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-06-18-collaboration-network/collaboration-network_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2022-07-03T22:39:11-07:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-06-18-reproducing-schotter-in-r/",
    "title": "Reproducing Schotter in R",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Abel Torres Espin",
        "url": {}
      }
    ],
    "date": "2022-06-18",
    "categories": [],
    "contents": "\r\nI have been captivated by creative coding and generative art for a\r\nwhile. After discovering some of the work by Antonio S.Chinchón and his\r\nblog Fronkonstin on using R and\r\nmathematics to create amazing art work, I decided to play around. Schotter (1968)\r\nis a famous piece by Georg Nees, a\r\npioneer in computer graphics and probably one of the first creative\r\ncoders.\r\nCHECK\r\nTHE ORIGINAL NOW\r\nTo pay tribute to Georg, I decided to implement a chunk of code that\r\nwould reproduce Schotter using R. There are probably several ways to\r\ncode this algorithm and I am sure several people have done this before.\r\nMy goal was to see if my current R knowledge could be use to program\r\nSchotter without checking other implementations and learn something on\r\nthe way.\r\nHere my thought process.\r\nThe libraries\r\nNowadays I use ggplot2\r\nfor every visualization, so I decided to utilize its amazing graphical\r\npowers. I also used dplyr\r\nalthough we could easily not use it here.\r\n\r\n\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\n\r\n\r\n\r\nBuilding a square\r\nThe easiest way to build a square in ggplot2 is probably using\r\ngeom_rect, however I did not find a way to rotate the geometry,\r\nso I decided to go for a polygon (geom_polygon) and control the\r\nposition of the 4 vertices outside ggplot. For that I created the\r\nfollowing function:\r\n\r\n\r\nsquare<-function(x0=1,y0=1, size=1, angle=0){\r\n  xor<-x0+size/2 #X origin (center of the square)\r\n  yor<-y0+size/2 #Y origin (center of the square)\r\n  \r\n  tibble(\r\n    x=c(x0,x0+size,x0+size,x0),\r\n    y=c(y0,y0,y0+size,y0+size)\r\n  )%>%mutate(x2=(x-xor)*cos(angle)-(y-yor)*sin(angle)+xor, #For rotation\r\n             y2=(x-xor)*sin(angle)+(y-yor)*cos(angle)+yor) #for rotation\r\n}\r\n\r\n\r\n\r\nWhere x0 and y0 control the\r\nposition of lower left (or other depending on the axis orientation)\r\nvertex, size the length of the sides and angle the amount of rotation\r\nfrom the original position. An x and y\r\nfor each vertex is set as a template relative to x0,\r\ny0 and size. In this way, we will be able to control\r\nthe position of the square in the canvas. x2 and\r\ny2 are set as the rotated coordinates for\r\nx and y, respectively. Ultimately, we\r\nwill use x2 and y2, that will be equal\r\nto x and y if angle is 0. I decided to\r\ngenerate new columns for the rotations so I can plot both, the original\r\nx, y and the rotated (I confess I had to brush up my trigonometry and\r\ngoogle a few things for this).\r\n\r\n\r\nggplot(square())+\r\n  geom_polygon(aes(x=x, y=y))+\r\n  coord_fixed()# just so that axis are equally spaced\r\n\r\n\r\n\r\n\r\nWe have our first square!! We can now play with fill and color\r\n\r\n\r\nggplot(square())+\r\n  geom_polygon(aes(x=x, y=y), fill=NA, color='black')+\r\n  coord_fixed()\r\n\r\n\r\n\r\n\r\nLet’s play with some parameters\r\n\r\n\r\nggplot(square(size=2,angle=pi/4))+#size 2 and rotate 45 degrees\r\n  geom_polygon(aes(x=x, y=y), fill=NA, color='black')+\r\n  geom_polygon(aes(x=x2, y=y2), fill=NA, color='red')+\r\n  coord_fixed()\r\n\r\n\r\n\r\n\r\nHow many squares we want?\r\nSchotter can be seen as a matrix of squares. A simple way to iterate\r\nthrough a matrix is to nest a loop inside a loop such that one loop goes\r\nthrough the rows and another through the columns.\r\nThe original Schotter has 12 columns and 24 rows (that I counted)\r\n\r\n\r\nn<-24*12\r\ndf.list<-list()\r\n  for (j in 1:24){ #iterate through the rows\r\n    for (i in 1:12){ #iterate through the columns\r\n      temp<-square(x=i, y=j) #create a square at column i and row j\r\n      df.list[[n]]<-temp #save the data frame with the square n on a list\r\n      n<-n-1\r\n    }\r\n  }\r\n\r\n\r\n\r\nThis chunk creates a data frame containing the x,\r\ny, x2 and y2 for 288\r\n(24x12) squares, that we can plotted. To plot each square independently\r\nas we will need for later, we need to add a new geom_polygon\r\nlayer for each square to a ggplot object. We could have done this in the\r\nprevious loops, but in the course of this mini project I have learned\r\nthat you can pass a list to ggplot. So, we can iterate though our list\r\nof square data frames using lapply and add a geom_polygon for\r\neach square (a bit slow on my friend).\r\n\r\n\r\nggplot() + \r\n  lapply(df.list, function(square_data) {\r\n    geom_polygon(data = square_data, aes(x = x2, y = y2), fill=NA, color='black')}\r\n  )+\r\n  coord_fixed()\r\n\r\n\r\n\r\n\r\nNow we have the grid of squares. At this point I have decided to deal\r\nwith the unnecessary elements of the plot. I generated a theme function\r\nsuch that I can easily change the background color.\r\n\r\n\r\ntheme_background<-function(color='white'){\r\n  theme(axis.ticks = element_blank(), axis.text = element_blank(),\r\n        panel.background = element_blank(), panel.grid = element_blank(),\r\n        plot.background = element_rect(fill = color),\r\n        strip.background = element_rect(fill=color),strip.text = element_blank(),\r\n        axis.title = element_blank(), legend.position = 'none')\r\n}\r\n\r\nggplot() + \r\n  lapply(df.list, function(square_data) {\r\n    geom_polygon(data = square_data, aes(x = x2, y = y2), fill=NA, color='black')}\r\n  )+\r\n  coord_fixed()+\r\n  theme_background()\r\n\r\n\r\n\r\n\r\nGenerative process\r\nGenerative art is define by wikipedia as:\r\nGenerative art refers to art that in whole or in part has been created with the use of an autonomous system. An autonomous system in this context is generally one that is non-human and can independently determine features of an artwork that would otherwise require decisions made directly by the artist. \r\nAn easy way to give autonomy to our code is by introducing\r\nrandomness. As we can appreciate from Schotter, both the position and\r\nthe angle of the squares seems to be more random and move away from the\r\n‘calm’ stage as we move through the rows. We will add this to our nested\r\nloops by generating a random displacement and rotation for each square\r\nthat increases by row.\r\n\r\n\r\nn<-24*12\r\ndf.list<-list()\r\n  for (j in 1:24){ #iterate through the rows\r\n    for (i in 1:12){ #iterate through the columns\r\n      displace<-runif(1,-j,j) #a random number is generated from a uniform distribution with min=-j and max=j\r\n      rotate<-runif(1,-j,j) #random number to rotate the square\r\n      temp<-square(x=i+displace, y=j+displace, angle=rotate) #create a square at column i and row j displaced by displace\r\n      df.list[[n]]<-temp #save the data frame with the square n on a list\r\n      n<-n-1\r\n    }\r\n  }\r\n\r\nggplot() + \r\n  lapply(df.list, function(square_data) {\r\n    geom_polygon(data = square_data, aes(x = x2, y = y2), fill=NA, color='black')}\r\n  )+\r\n  coord_fixed()+\r\n  theme_background()\r\n\r\n\r\n\r\n\r\nUps! Something isn’t quite right (although I like it!). I think is\r\nbecause j, that controls how much displacement and rotation can\r\nbe added get to be too big in respect to x and\r\ny. So, let’s control for that. For that I divided\r\nj by 40 in the case of displace and by 100 in the case of\r\nrotate. I found these numbers by playing around. I also flipped y\r\naxis.\r\n\r\n\r\nn<-24*12\r\ndf.list<-list()\r\n  for (j in 1:24){ #iterate through the rows\r\n    for (i in 1:12){ #iterate through the columns\r\n      displace<-runif(1,-j/40,j/40) #a random number is generated from a uniform distribution with min=-j and max=j\r\n      rotate<-runif(1,-j/100,j/100) #random number to rotate the square\r\n      temp<-square(x=i+displace, y=j+displace, angle=rotate) #create a square at column i and row j displaced by displace\r\n      df.list[[n]]<-temp #save the data frame with the square n on a list\r\n      n<-n-1\r\n    }\r\n  }\r\n\r\nggplot() + \r\n  lapply(df.list, function(square_data) {\r\n    geom_polygon(data = square_data, aes(x = x2, y = y2), fill=NA, color='black')}\r\n  )+\r\n  coord_fixed()+\r\n  theme_background()+\r\n  scale_y_reverse()\r\n\r\n\r\n\r\n\r\nAnd voila! we have coded a generative algorithm to produce\r\nsomething like Schotter. Because the randomness, every time we run the\r\ncode, the result will be different.\r\nNow we wrap the code in a function so that we can easily change\r\nparameters\r\n\r\n\r\nSchotter<-function(ncol_s=12, nrow_s=24, control_dis=40, \r\n                   control_rot=100, fill_s=NA, color_s='black', alpha_s=0.2, \r\n                   back_color='white'){\r\nn<-ncol_s*nrow_s\r\ndf.list<-list()\r\n  for (j in 1:nrow_s){\r\n    for (i in 1:ncol_s){\r\n      displace<-runif(1,-j/control_dis,j/control_dis)\r\n      rotate<-runif(1,-j/control_rot,j/control_rot)\r\n      temp<-square(x=i+displace, y=j+displace, angle=rotate)\r\n      temp$s<-rep(n)\r\n      df.list[[n]]<-temp\r\n      n<-n-1\r\n    }\r\n  }\r\n\r\nggplot() + \r\n  lapply(df.list, function(square_data) {\r\n    geom_polygon(data = square_data, aes(x = x2, y = y2),\r\n                 alpha=alpha_s, fill=fill_s, color=color_s)}\r\n  )+\r\n  theme_background(color = back_color)+\r\n  coord_fixed()+\r\n  scale_y_reverse()\r\n}  \r\n\r\n\r\n\r\nWe can now play!\r\n\r\n\r\nSchotter()\r\n\r\n\r\n\r\n\r\n\r\n\r\nSchotter(fill='firebrick', color='orange', nrow_s = 48, control_dis = 20)\r\n\r\n\r\n\r\n\r\nThe End\r\nThis is all for now. There are a few ways the code could be improved,\r\nbut I am happy with the result and I have had fun and learned something\r\nnew!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-06-18-reproducing-schotter-in-r/reproducing-schotter-in-r_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-06-19T07:46:36-07:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-01-14-from-wet-lab-to-data-science/",
    "title": "From wet lab to data science",
    "description": "How I think experimental researchers are data scientist",
    "author": [
      {
        "name": "Abel Torres Espín",
        "url": {}
      }
    ],
    "date": "2022-01-14",
    "categories": [
      "Data science"
    ],
    "contents": "\r\nNot while ago, I introduced myself as a data scientist, but it has not always been like that. Let me tell you how a biologist like me gets to that and how I believe any researcher can. I am a biomedical researcher and I spent the first 10 years of my career performing lab experiments, what is known as “wet lab”. At some point I realized that doing experiments was not that exciting anymore (a story for another day), but I was not ready to stop doing research, so I decided to shift focus to a more data-intensive work. For the last 3 years I have moved from a pure wet lab work to a pure dry or computational professional activity. During this time, I have learned a lot of new stuff, but I have also got to the conclusion that researchers have a good head start to transition to data science and related work. After all, data is the prima matter for contemporary research. Don’t make me wrong, is not an easy path, but if you are reading this because you are thinking about it, let me convince you that your research knowledge and skills are not that far from those needed to start a career in data science.\r\n\r\nNOTE: If you are a computational researcher or your work is heavy on the quantitative site, you probably already made the connections I am making here and you probably already call yourself a data scientist. I have the perception that there is an increase in the number of researchers that has some working experience on computational methods, but yet are not fully aware of it.\r\n\r\nWe scientist have a lot of work qualities, but acknowledging our transferable skills is not one of them. So let me illustrate my point. First thing first, researchers like to ask questions about the unknown and generate hypothesis about how they think things work. And how do we answer those questions? We turn into data. No matter your field, if you are doing research today, you probably have had to deal with data. What data scientist do? They ask questions, using data to answer them. Not that different so far. Let’s dissect this process and see how researcher’s skills can translate to data science.\r\nAsk your question: We went over this already, all starts with some questions, whether you do research or business analytics, you always want to know something new and researchers sick the limits of knowledge by definition.\r\nGather your data: Here researchers excel! After years of research work, I am pretty sure you know your ups and downs about doing literature review, designing an experiment tailored to answer your question, and painstikly performed experiment/s and data collection. Data scientist do the same but using other tools. Actually, depending on their work, data scientist might not do any data gathering themselves. Researchers might also get the data already collected, but if you have done research for a while, you would probably be asking a lot of questions on how the data came to be.\r\nBuild your dataset: Do you remember all those hours entering numbers in a spreadsheet? You are halfway there then. In my experience, here is where researchers might lack basic concepts such as data structures, relational databases, metadata or data dictionary. So next time you enter data in any sort of digital data keeper, you could start by tidying your dataset (Data organization in spreadsheets) and build a data dictionary for it.\r\nData wrangling: This is a jargon to evoke any process related to joint, transform, re-organize, summarize and re-format datasets. While well formatted data for storage is important, almost every time you want to perform a specific analysis, you will have to make changes in the data structure. Most researchers probably know their way through spreadsheets or some stats software to bring the data to the required format for analysis.\r\nExplore data: Meeting after meeting you show summary statistics and graphs to lab mattes, supervisors and collaborators. That is really it, you already know how to do this. Produce some summaries and visuals to start getting a sense of the data, check for potential errors and trends, etc.\r\nData analysis: If you have performed and analyzed research experiments, you probably navigated things like descriptive statistics, hypothesis testing, t-test, analysis of variance (ANOVA), p-values, parametric and non-parametric test, linear regression, correlation, normal distribution, to say a few. So you know more analysis and statistics that you might think. Although data analysis can be very complex and sophisticated in data science, researchers are very well equipped to do a lot of analytical task.\r\nGet insights: Interpret your results. As a researcher, I believe this is one of your powers. Communication, data visualization and storytelling: If anyone things that researchers are these robots that communicate between them using dull technical reports full of numbers and lacking literary elements, will be very surprised at reading a scientific manuscript. Yes, these documents have more jargon and terminology than the average read, but nowadays manuscript and grant writing is an art of effective communication. How to say as much as possible, in the minimum space, for a broad audience (editors, reviewers and a broad reach of researchers), and at the same time captivate and avoid boresome. You know how to sell your insights. An although I think there is still a lot of space for improvement in scientific communication using viz, you know how to make pretty tables, plots, figures, posters and slide presentations.\r\nLearn new tools: there are a lot of tools in data science that researchers don’t know about. And the number keeps increasing at a scary paste. But so it is in research. Both researchers and data scientist are used to the need of keeping up with new methods and learning some of them in detail. Don’t let this scare you.\r\nAs you can see, researchers and data scientist share more than what you might think at first glance. Sure you will have to learn a lot, and sure will get frustrating some times, but if research has taught me something is how to constantly learn and how to endure these frustrations and transform them into growth. If you are a researcher that have considered the idea of changing to a more data-centric role, I hope I have convinced you that you are not starting from nothing and I hope I can ease a bit the anxiety of confronting a change like this.\r\n\r\nRECOMMENDATION: I truly believe researchers have a lot of transferable skills to data science. If I might give a recommendation for those wanting to do the transition, I would say “learn the terminology and start learning some coding”. Once you know how things are called in data science, you will realize you know more than what you think you know. Code will make your life easier even if you decide to stay in research after all.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-01-14T08:45:17-08:00",
    "input_file": {}
  }
]
