[
  {
    "path": "posts/2024-12-16-bipartite-network-of-authors-and-research-themes/",
    "title": "Who are you among all of us?",
    "description": "A bipartite network of an author group and research topics",
    "author": [
      {
        "name": "Abel Torres Espin",
        "url": {}
      }
    ],
    "date": "2025-02-18",
    "categories": [
      "Networks"
    ],
    "contents": "\r\nIn spring of 2023, I gave my faculty application job presentation for what would\r\nend up being my current appointment at the University of Waterloo. At the end of\r\nmy presentation, I had a final section showing a plot like this one:\r\nbipartite networkThis is a bipartite network of two types of entities: the group of faculty members at the School of Public Health Sciences and a collection of research topics (in red) derived from the abstracts published by the faculty members (in blue), and myself (in green) for the last 10 years. The nodes of the network represent each of the faculty members and the research topics, and the edges (the links between nodes) mark whether a given faculty member is associated with a research topic given their published work.\r\n\r\nThe names of the faculty have been de-identified, but I used their real names in my presentation.\r\n\r\nI hope the goal of having something like this is clear. First, I did this for myself, wanting to know where and how I belong among the faculty group (regarding my research). But then I thought it would be a great part of my presentation to show how do I fit, what new I bring, and my skills in a fun and original way. I remember getting good laughs and jiggles when I reached this part of the talk. I wanted to post how I did it in case it is helpful to anyone. So, let’s get coding!\r\nQuick overview\r\nThis is what we need:\r\nCoding. A coding environment. I did this in R, so this post will show the R code, but it can be done in Python or other languages, too. Pick your weapon!\r\nPeople. A list of all authors we want to include in the network. In my case, that was all faculty members at the School, whose names and links to their Google Scholar pages I gathered manually. You could also extract that information from a set of papers or a bibliometric search like in [link]. It is important that you get the preferred publishing name, as we will use it to extract publication records for each author.\r\nPapers. A way to collect the abstracts for all authors for a given period of time. I used two sources: Google Scholar and PubMed. It is possible that some publications are not indexed or listed there, but the chances of missing a paper are probably small. Besides, this is not meant to be the most exhaustive search!\r\nResearch topics. We can consider extracting research topics from paper titles and abstracts in several ways. You could do a painful manual process of assigning topics to papers and then aggregate them… I had around 4000 publications to process, and this is a data science blog… so I used a bit of computational help through Natural Language Processing (NLP). Clustering text is a common NLP task to find similarities between documents. I used topic modeling to find those clusters and determine topics. You could also group papers based on keywords and MeSh terms.\r\n\r\nUsing Large Language Models.\r\nWhen I was preparing this for my talk, I didn’t have the time to learn how to set up an LLM for this (I know now!), but you can definitely use one to group papers and extract research topics, too.\r\n\r\nPlotting. We need a way to visualize the relationships between authors and research topics. I choose a bipartite network, but you can summarize the data in other ways like a chord plot\r\nLibraries\r\nHere the list of all the libraries I used\r\n\r\n\r\nlibrary(scholar) # For parsing Google Scholar information from authors\r\nlibrary(tidyverse) # For data wrangling and other tasks\r\nlibrary(easyPubMed) # For querying PubMed\r\n\r\n# dealing with text and topic modeling\r\nlibrary(textclean) # useful text cleaning functions\r\nlibrary(tm) # text mining functions such as data structures\r\nlibrary(tidytext) # more text cleaning and mining functions\r\nlibrary(topicmodels) # for topic modeling functions\r\nlibrary(textmineR) # further text mining functions\r\n\r\n# plotting\r\nlibrary(igraph) # Extensive graph library\r\nlibrary(ggnetwork) #ggplot extension for graph plotting\r\n\r\n# Parallel\r\nlibrary(parallel) # for parallel processing\r\n\r\n\r\nExtract authors and abstract information\r\nWe start by extracting the list of papers from each author we want. The raw_df dataset was created manually and contains each individual’s name, research interest, and Google ID. I collected this information by painstakingly going through each faculty member’s profile!\r\n\r\n\r\n# Filter the raw_df dataset containing names, research interests, and Google IDs.\r\ng_id_df<-raw_df%>%\r\n  filter(!is.na(g_scholar))\r\n\r\n# API call to get publications ids for each author\r\npubs<-lapply(g_id_df$g_scholar, get_publications)\r\n\r\n# Add names of the author to each df\r\npubs_name<-lapply(1:length(pubs), function(i){\r\n  pubs[[i]]$name<-g_id_df$name[i]\r\n  pubs[[i]]$id<-g_id_df$g_scholar[i]\r\n  pubs[[i]]\r\n})\r\n\r\n# collapse to a single dataset and save in disk\r\npubs_df<-bind_rows(pubs_name)\r\n\r\nsaveRDS(pubs_df, file = \"pubs_df.Rds\")\r\n\r\n\r\nNow, we can extract the info per paper. The Google Scholar URL connection has a\r\nvery low time/frequency limit for making requests, which makes it difficult to\r\nextract all the information per author. However, we can use the PubMed API to query per paper.\r\n\r\n\r\n# Filter for the last 10 years (from 2013 to 2023) to keep the network relevant\r\npubs_df_f<-pubs_df%>%\r\n  filter(year>=2013)\r\n\r\n# Create a set of clusters for parallel computing\r\ncl<-makeCluster(14)\r\nparallel::clusterEvalQ(cl, expr = {\r\n  library(easyPubMed)\r\n})\r\n\r\nclusterExport(cl, varlist = \"pubs_df_f\")\r\n\r\n# Iterate the paper list in parallel to extract all information (title, abstract, etc) from PubMed\r\npubmed_data<-pblapply(1:nrow(pubs_df_f), function(i){\r\n  pubmed_ids<-get_pubmed_ids_by_fulltitle(pubs_df_f$title[i])\r\n  if(pubmed_ids$Count==0){\r\n    return(NA)\r\n  }else{\r\n    fetch_pubmed_data(pubmed_ids, format = \"medline\" )\r\n  }\r\n}, cl = cl)\r\n\r\n# Save for later use. This process can take a while. Saving the results so we don't need to rerun the API calls\r\nsaveRDS(pubmed_data, \"pubmed_data.Rds\")\r\n\r\n\r\nProcess the abstracts\r\nNow, we can process each abstract. We do two things. First, isolate the abstract for each paper. Then, we clean the text to facilitate downstream analysis. This is a common task in natural language processing. For example, we transform all text to lowercase. Both functions for extraction (extract_abs()) and text cleaning (clean_text()) are shown at the end of the document.\r\nExtract abstracts\r\n\r\n\r\npubs_df_f$clean_abs<-NA\r\nfor (a in 1:length(pubmed_data)){\r\n  cat(\"\\r\", a)\r\n  pubs_df_f$clean_abs[a]<-extract_abs(pubmed_data[[a]])\r\n}\r\n\r\n\r\nClean the abstracts and the titles\r\n\r\n\r\npubs_df_f$clean_abs<-clean_text(pubs_df_f$clean_abs)$t\r\npubs_df_f$clean_title<-clean_text(pubs_df_f$title)$t\r\n\r\nsaveRDS(pubs_df_f, \"pubs_df_f.Rds\")\r\n\r\n\r\nNow, we need to add my abstracts to the list. I could have added my Google Scholar information in the process above and processed as every other author, but I did not.\r\n\r\n\r\nATE_pubs_ids<-get_pubmed_ids(\"abel torres espin\")\r\nATE_pubmed_data<-fetch_pubmed_data(ATE_pubs_ids, format = \"medline\")\r\n\r\n# get PMIDs\r\nstarts<-which(str_detect(ATE_pubmed_data, \"^PMID\"))\r\npmid<-ATE_pubmed_data[starts]\r\n\r\n#extract papers\r\nstarts<-c(starts, length(ATE_pubmed_data))\r\nATE_pubmed_data_list<-list()\r\n\r\nfor(i in 1:(length(starts)-1)){\r\n  ATE_pubmed_data_list[[i]]<-ATE_pubmed_data[starts[i]:(starts[i+1]-1)]\r\n}\r\n\r\npubmed_data<-readRDS(\"pubmed_data.Rds\")\r\npubmed_data<-append(pubmed_data,ATE_pubmed_data_list)\r\n\r\n# Save again with the addition of my abstracts\r\nsaveRDS(pubmed_data, \"pubmed_data.Rds\")\r\n\r\n\r\nAdding my information on the pub_df_f dataframe\r\n\r\n\r\nATE_pubmed_data_list<-readRDS(\"ATE_pubmed_data_list.Rds\")\r\nATE_pub_df<-data.frame(name = \"Abel Torres Espin\",\r\n                       clean_abs = unlist(lapply(ATE_pubmed_data_list, extract_abs)),\r\n                       pubid = pmid)\r\nATE_pub_df$clean_abs<-clean_text(ATE_pub_df$clean_abs)$t\r\npubs_df_f<-bind_rows(pubs_df_f, ATE_pub_df)\r\n\r\nsaveRDS(pubs_df_f, \"pubs_df_f.Rds\")\r\n\r\n\r\nText clustering\r\nNow that we have all the data that we need, we can start the process of clustering all abstracts given their similarities.\r\n\r\n\r\n# Read previous dataframe\r\npubs_df_f<-readRDS(\"pubs_df_f.Rds\")\r\n\r\n# Filter out those that are corrections or the abstract is empty\r\npubs_df_clustering<-pubs_df_f%>%\r\n  filter(!str_detect(clean_abs, \"this corrects the article\")|is.na(clean_abs))\r\n\r\n#If the abstract is empty, use the title. Also, remove some highly common words. \r\n#This is often performed because otherwise, these words might drive the whole clustering \r\n#solution, but they are likely present in each abstract\r\n\r\npubs_df_clustering<-pubs_df_clustering%>%\r\n  mutate(clean_abs2 = ifelse(is.na(clean_abs), clean_text(title)$t, clean_abs),\r\n         clean_abs2 = removeWords(clean_abs2, c(\"health\",\"care\",\r\n                                     \"use\",\"studi\", \"data\", \"associ\", \"result\",\r\n                                     \"research\")))\r\npubs_df_clustering<-pubs_df_clustering%>%\r\n  filter(!str_detect(clean_abs2, \" des | de | la \"))\r\n\r\n\r\nNow, we can create a document term matrix, a data structure that stores the documents by tokens. In this case, we use an n-gram window of 1, which means tokens correspond to a single word.\r\n\r\n\r\n# create document term matrix (tokens)\r\nset.seed(55)\r\ndtm_clean_abs<-CreateDtm(pubs_df_clustering$clean_abs2,pubs_df_clustering$pubid,\r\n                    ngram_window = c(1,1),\r\n                    stopword_vec = stopwords(\"en\"),\r\n                    verbose = F,\r\n                    stem_lemma_function = function(x)\r\n                      SnowballC::wordStem(x, \"en\"))\r\n\r\n## Save so that this doesn't need to run every time\r\nsaveRDS(dtm_clean_abs, \"dtm_clean_abs.Rds\")\r\n\r\n\r\nWe create a frequency matrix counting the times each token shows up across abstracts. We do some further filtering and calculate the cosine similarity between abstracts. This is a measure of how similar the documents are given the frequency of words used, with the understanding that closer abstracts would tend to use more similar words. Finally, we perform hierarchical clustering on the distance (1 - similarity).\r\n\r\n\r\ndtm_clean_abs<-readRDS(\"dtm_clean_abs.Rds\")\r\n\r\ntf_mat <- TermDocFreq(dtm_clean_abs)\r\n\r\n## filter for tokens in more than one document\r\ntf_mat_f<-tf_mat%>%\r\n  filter(doc_freq>1 & doc_freq<663)%>%\r\n  filter(!term%in%c(\"n\", \"p\", \"patient\",\r\n                    #\"â\",\r\n                    \"factor\", \"increas\", \"use\", \"intervent\",\r\n                    \"studi\", \"result\", \"\"))\r\n\r\n# TF-IDF and cosine similarity\r\ntfidf <- t(dtm_clean_abs[ , tf_mat_f$term ]) * tf_mat_f$idf\r\ntfidf <- t(tfidf)\r\n\r\ncsim <- tfidf / sqrt(rowSums(tfidf * tfidf))\r\ncsim <- csim %*% t(csim)\r\n\r\n# calculate distance\r\ncdist <- as.dist(1 - csim)\r\ncdist[is.na(cdist)]<-1\r\n\r\n# Hierarchical clustering\r\nhc <- hclust(cdist, method = \"ward.D2\")\r\nplot(hc, labels = FALSE, hang = -1)\r\n\r\n\r\n\r\nExplain clusters\r\nNow that we have clusters of documents based on their similarity, we can attempt to explain what each cluster means by checking the most frequent words in the documents for each cluster.\r\nAfter extracting that list of words per cluster, I manually named each cluster, given the themes I considered appropriate from the list. You can see the five most common words per cluster\r\n\r\n\r\ntfidf_df<-as.data.frame(as.matrix(tfidf))\r\ntfidf_df$clust<-cutree(hc, 60)\r\n\r\ntfidf_df_sum<-tfidf_df%>%\r\n  group_by(clust)%>%\r\n  summarise(across(everything(), \\(x) mean(x,na.rm = TRUE)))%>%\r\n  pivot_longer(cols = -clust)\r\n\r\ntfidf_df_sum_f<-tfidf_df_sum%>%\r\n  group_by(clust)%>%\r\n  slice_max(value, n = 5)%>%\r\n  filter(value>0)\r\n\r\n#write.csv(tfidf_df_sum_f, \"tfidf_df_sum_f2.csv\")\r\n\r\ntfidf_df_sum_f %>%\r\n  mutate(name = reorder_within(name, value, clust)) %>%\r\n  ggplot(aes(value, name, fill = factor(clust))) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~ clust, scales = \"free\") +\r\n  scale_y_reordered()\r\n\r\n\r\n\r\nGraphs\r\nNow we can plot a graph by associating each document and the author of it to a cluster/\r\n\r\n\r\nset.seed(123)\r\n\r\n# Format the data for a network\r\ncluster_names<-read.csv(\"tfidf_df_sum_f2.csv\", na.strings = \"\")%>%\r\n  mutate(clust_level2 = ifelse(is.na(clust_level2), clust_name, clust_level2))%>%\r\n  group_by(clust)%>%\r\n  summarise(clust_name = unique(clust_name),\r\n            clust_level2 = unique(clust_level2))\r\n\r\npubs_df_clustering$clust <- cutree(hc, 60)\r\n\r\npubs_df_clustering2<-pubs_df_clustering%>%\r\n  left_join(cluster_names%>%\r\n              select(clust, clust_level2))%>%\r\n  filter(!is.na(clust_level2))\r\n\r\npubs_df_clustering2$name<-str_trim(pubs_df_clustering2$name)\r\n\r\nnet_df<-pubs_df_clustering2%>%\r\n  select(name, clust_level2)\r\n\r\n# Extract a graph\r\ng<-graph_from_data_frame(net_df, directed = F)\r\n\r\n# Create the bipartite graph and format as a ggnetwork for plotting with ggplot\r\nV(g)$type <- bipartite_mapping(g)$type\r\nV(g)$size <- degree(g)\r\ng<-simplify(g)\r\ngg_net<-ggnetwork(g)\r\n\r\n# Add further information on the network dataset\r\ngg_net<-gg_net%>%\r\n  mutate(type2 = case_when(\r\n    name == \"Abel Torres Espin\"~\"me\",\r\n    type == FALSE~\"name\",\r\n    type == TRUE~\"cluster\"\r\n  ),\r\n  size2 = ifelse(type == FALSE, 1, 1.1),\r\n  line_size = ifelse(type2 == \"me\", 0.15, 0.1))%>%\r\n  group_by(name)%>%\r\n  mutate(name = case_when(\r\n    type == FALSE~paste0(sample(letters, 5, replace = T), collapse = \"\"),\r\n    TRUE~name\r\n  ))\r\n\r\n#Finally, plot the network\r\nggplot(gg_net, aes(x = x, y = y, xend = xend, yend = yend)) +\r\n  geom_edges(aes(color = type2), alpha=0.4,\r\n             curvature = 0.1, linewidth = 0.2) +\r\n  geom_nodes(aes(color = type2, shape = type2))+\r\n  geom_nodelabel_repel(aes(label = name, fill = type2),\r\n                       show.legend = F, force = 0.04,size = 6)+\r\n  scale_color_manual(values = c(\"grey\",\"green3\", \"grey\"))+\r\n  scale_fill_manual(values = c(\"lightpink2\", \"green3\", \"steelblue1\"))+\r\n  theme_blank()+\r\n  theme(legend.position = \"none\")\r\n\r\n\r\n# ggsave(filename = \"bipartite_network.png\", width = 16, height = 12)\r\n\r\n\r\nHere another way to plot the same network.\r\n\r\n\r\ngg_net<-ggnetwork(g, layout = igraph::layout_as_bipartite(g))\r\ngg_net<-gg_net%>%\r\n  mutate(type2 = case_when(\r\n    name == \"Abel Torres Espin\"~\"me\",\r\n    type == FALSE~\"name\",\r\n    type == TRUE~\"cluster\"\r\n  ),\r\n  size2 = ifelse(type == FALSE, 1, 1.1),\r\n  line_size = ifelse(type2 == \"me\", 0.15, 0.1))%>%\r\n  group_by(name)%>%\r\n  mutate(name = case_when(\r\n    type == FALSE~paste0(sample(letters, 5, replace = T), collapse = \"\"),\r\n    TRUE~name\r\n  ))\r\n\r\nggplot(gg_net, aes(x = x, y = y, xend = xend, yend = yend)) +\r\n  geom_edges(aes(color = type2), alpha=0.4,\r\n             curvature = 0.1, linewidth = 0.2) +\r\n  geom_nodes(aes(color = type2, shape = type2))+\r\n  geom_nodelabel_repel(aes(label = name, fill = type2),\r\n                       show.legend = F, force = 0.04,size = 4)+\r\n  scale_color_manual(values = c(\"grey\",\"green3\", \"grey\"))+\r\n  scale_fill_manual(values = c(\"lightpink2\", \"green3\", \"steelblue1\"))+\r\n  theme_blank()+\r\n  theme(legend.position = \"none\")\r\n\r\n\r\n\r\nAnd that it is! I hope this is useful so someone. I cannot say I got the job because it, but I can tell I got some good smiles in the room! I achieved what I wanted: an original way to show my skills, how I am as a researcher, and how I fit among all these incredible people.\r\n\r\n\r\n#Function to extract abstracts\r\nextract_abs<-function(x){\r\n  if (is.null(x)) return (NA)\r\n  start<-which(str_detect(x, \"^AB  -\"))\r\n  if (length(start) == 0){\r\n    return(NA)\r\n  }\r\n  count = start + 1\r\n  while( TRUE ){\r\n    if (str_detect(x[count], \"^ \")){\r\n      count = count +1\r\n    }else{\r\n      break()\r\n    }\r\n  }\r\n  abstract<-paste0(x[start:(count-1)],collapse = \"\")\r\n  abstract<-str_remove(abstract,\"^AB  -\")\r\n  return(abstract)\r\n}\r\n# Function for cleaning\r\nclean_text<- function(t){\r\n  t <- as.character(t)\r\n  t <- t %>%\r\n    str_to_lower() %>%  # convert all the string to low alphabet\r\n    replace_contraction() %>% # replace contraction to their multi-word forms\r\n    replace_internet_slang() %>% # replace internet slang to normal words\r\n    replace_word_elongation() %>% # replace informal writing with known semantic replacements\r\n    replace_number(remove = T) %>% # remove number\r\n    replace_date(replacement = \"\") %>% # remove date\r\n    replace_time(replacement = \"\") %>% # remove time\r\n    str_remove_all(pattern = \"[[:punct:]]\") %>% # remove punctuation\r\n    str_squish() %>% # reduces repeated whitespace inside a string.\r\n    str_trim() # removes whitespace from start and end of string\r\n  return(as.data.frame(t))\r\n}\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-12-16-bipartite-network-of-authors-and-research-themes/bipartite-network_files/figure-html5/unnamed-chunk-15-1.png",
    "last_modified": "2025-02-18T22:47:59-05:00",
    "input_file": "bipartite-network.knit.md",
    "preview_width": 2880,
    "preview_height": 2304
  },
  {
    "path": "posts/2022-06-18-collaboration-network/",
    "title": "Co-author Network",
    "description": "Building a network of co-authors in R",
    "author": [
      {
        "name": "Abel Torres Espin",
        "url": {}
      }
    ],
    "date": "2023-03-18",
    "categories": [
      "Networks"
    ],
    "contents": "\r\nThis post will show you how you can extract and visualize your network of co-authors. You can also find the code for this post on GitHub.\r\nLuckily for us, the hard work needed for this has been already programmed by amazing people out there, so we will be using these packages:\r\n\r\n\r\nlibrary(easyPubMed) #API connection with pubmed\r\nlibrary(tidyverse) #To facilitate some data wrangling\r\nlibrary(igraph) #To build the graph\r\nlibrary(networkD3) #Interactive networks\r\nlibrary(ggnetwork) #for static networks\r\n\r\n\r\nExtracting Pubmed information\r\nThe easyPubMed package can be used for… yes, you guessed right… easily extract information from Pubmed. So, the first thing to do is to search us and extract the list of authors in our papers from the ‘medline’ format, tagged as ‘AU’.\r\n\r\n\r\nmy_entrez_id <- get_pubmed_ids('Torres-Espin A')\r\nmy_author_txt <- fetch_pubmed_data(my_entrez_id, format = \"medline\")\r\nauthors<-my_author_txt[str_detect(my_author_txt, \"^AU \")]\r\ncoauthors_list<-unique(str_remove_all(authors, '^AU  - '))\r\nhead(coauthors_list)\r\n\r\nsort(coauthors_list)\r\n\r\n\r\nNote that we use unique because we might have more than one paper with the same author. We do this because now we are going to search the list of co-authors to get the relationship of publication between them. If you only want to get the first level connections of each co-author with you, then do not need to collapse the coauthors_list.\r\nNow let’s create a data frame containing the list of co-authors for each author on our list of co-authors. We can do that by iterating through the same code we did before and saving the results in a list of data frames. We will stack the list of data frames at the end by calling rbind. If you know some R you probably know that this code can be collapsed in fewer lines and be more efficient using apply. I like using loops from time to time!\r\n\r\n\r\nauthor_list<-list()#define the list to save the data frames\r\n\r\nfor (i in 1:length(coauthors_list)){\r\n  my_entrez_id <- get_pubmed_ids(coauthors_list[i])\r\n  my_author_txt <- fetch_pubmed_data(my_entrez_id, format = \"medline\")\r\n  \r\n  if (length(my_author_txt)==0) next()\r\n  \r\n  authors<-my_author_txt[str_detect(my_author_txt, \"^AU \")]\r\n  authors<-str_remove_all(authors, '^AU  - ')\r\n  author_list[[i]]<-data.frame(co_authors=authors, author=coauthors_list[i])\r\n}\r\nauthor_df<-do.call(rbind, author_list)#stack data frames\r\n\r\n\r\nWe can save this for later use so that we do not need to do the search again!\r\n\r\n\r\nsaveRDS(author_df, \"author_df.Rds\")\r\n\r\n\r\nThe next step is filtering our data frame to keep only the co-authors of our co-authors that are also our co-authors (this is getting tricky…). You could leave the co-authors of your co-authors that you have not published with, but that can make the graph quite big and not useful for plotting. After filtering we create a new variable with the joint string between co-author and author, so that we can count how many times that joint happens.\r\n\r\n\r\nauthor_df<-author_df[author_df$co_authors%in%author_df$author,]%>%\r\n  mutate(co_authors = ifelse(co_authors ==\"Torres Espin A\", \"Torres-Espin A\", co_authors),\r\n         author = ifelse(author ==\"Torres Espin A\", \"Torres-Espin A\", author))%>%\r\n  mutate(co_edge=paste(co_authors, author, sep = '_'))%>%\r\n  group_by(co_edge)%>%\r\n  summarise(count=n())%>%\r\n  separate(co_edge, c('co_authors','author'),sep = '_')\r\n\r\nauthor_df<-author_df%>%\r\n  group_by(co_authors, author)%>%\r\n  summarise(count = sum(count))\r\n\r\n\r\nWe are ready to create the graph from the data frame. We specify the weight of the edges (using E) as the count of the times two authors published together. Note that in our count data ‘author 1-author 2’ counts different than ‘author 2-author 1’ for the same paper, so we are counting each article twice. That is not a problem since we are considering a undirected graph. We just need to remember that we will be representing each article 2 times. I did not care because graphically does not matter. Now, we plot the graph (very rough!)\r\n\r\n\r\nauthor_g<-graph_from_data_frame(author_df,directed = F)\r\nE(author_g)$weight<-author_df$count\r\nauthor_g<-simplify(author_g)\r\n#layout_nicely(author_g)\r\nplot(author_g)\r\n\r\n\r\n\r\nLast step is to make the graph pretty and play with the parameters.\r\nFor a good network visualization tutorial in R check [this post]( https://mr.schochastics.net/material/netvizr/)\r\nUPDATE\r\nI decided to use the networkD3 package for computational efficiency. In the initial version of this post I used the visNetwork package, which I find an amazing tool, so I recommend you explore it if you use networks.\r\n\r\n\r\ngroup<-rep(1, length(V(author_g)))\r\ngroup[377]<-2\r\n\r\nauthor_d3 <- igraph_to_networkD3(author_g, group = as.character(group))\r\n\r\nforceNetwork(Links = author_d3$links, Nodes = author_d3$nodes, \r\n             Source = 'source', Target = 'target', NodeID = 'name', \r\n             Group = \"group\", linkColour = \"lightgrey\", \r\n             colourScale =JS('d3.scaleOrdinal().domain([\"1\", \"2\"]).range([\"#ADD8E6\", \"#FF6600\"])'), \r\n             opacity = 0.9)\r\n\r\n\r\n\r\nCode from initial post\r\n\r\n\r\nvis_g<-toVisNetworkData(author_g) #coverts the igraph object to visNetwork\r\nvis_g$edges$value<-vis_g$edges$weight #specify the values for eadges as the weights (number of papers for pairs)\r\n\r\n#vis_g$nodes$title<-co_authors_abel$author_name #name of the nodes\r\n\r\n#plot the network  \r\nvisNetwork(nodes = vis_g$nodes, edges = vis_g$edges, height = \"400px\", width = '500px')%>%\r\n  visNodes(size = 10, shape='cicle', mass=2,font=list(color='white'),fixed = F) %>%\r\n  visEdges(length = 10,physics = T, smooth = list(enabled=T))%>%\r\n  visLayout(randomSeed = 666)%>%\r\n  visOptions(highlightNearest = list(hover = T), \r\n             nodesIdSelection = list(enabled = TRUE))%>%\r\n  visPhysics(stabilization = T, minVelocity = 10, maxVelocity = 25)\r\n\r\n\r\nOr we can use the ggnetwork package for static networks. It is much faster to plot and integrates with ggolot2!\r\n\r\n\r\nnetwork<-ggnetwork(author_g, layout = igraph::layout.fruchterman.reingold(author_g))\r\nnetwork<-network%>%\r\n  group_by(name)%>%\r\n  mutate(node_size = n())\r\n\r\nggplot(network, \r\n       aes(x = x, y = y, xend = xend, yend = yend)) +\r\n  geom_edges(size = 0.6, color = \"grey50\", curvature = 0.1, alpha= 0.1, \r\n             show.legend = F) +\r\n  geom_nodes(aes(size = node_size),show.legend = F, color = \"black\",\r\n             fill = \"steelblue1\",\r\n             shape = 21) +\r\n  # geom_nodelabel(aes(label = name, fill = group), size = 2)+\r\n  # geom_nodes(aes(size = weight+weight*2), show.legend = F, color = \"steelblue\", alpha = 0.5) +\r\n  theme_blank()\r\n\r\n\r\nggsave(\"network_points.png\", width = 15, height = 10)\r\n\r\n\r\nDONE!\r\nNow you can save the network as html using the saveNetwork function. For the network in my co-authors section, I did some extra processing steps to consider that some times same author might publish with slightly different names. I also added some groups and colors. I would let you be creative.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-06-18-collaboration-network/collaboration-network_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2024-12-16T17:32:40-05:00",
    "input_file": "collaboration-network.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-06-18-reproducing-schotter-in-r/",
    "title": "Reproducing Schotter in R",
    "description": "Recreating Schotter by Georg Nees in R",
    "author": [
      {
        "name": "Abel Torres Espin",
        "url": {}
      }
    ],
    "date": "2022-06-18",
    "categories": [
      "Creative coding"
    ],
    "contents": "\r\nI have been captivated by creative coding and generative art for a while. After discovering some of the work by Antonio S.Chinchón and his blog Fronkonstin on using R and mathematics to create amazing art work, I decided to play around. Schotter (1968) is a famous piece by Georg Nees, a pioneer in computer graphics and probably one of the first creative coders.\r\nCHECK THE ORIGINAL NOW\r\nTo pay tribute to Georg, I decided to implement a chunk of code that would reproduce Schotter using R. There are probably several ways to code this algorithm and I am sure several people have done this before. My goal was to see if my current R knowledge could be use to program Schotter without checking other implementations and learn something on the way.\r\nHere my thought process.\r\nThe libraries\r\nNowadays I use ggplot2 for every visualization, so I decided to utilize its amazing graphical powers. I also used dplyr although we could easily not use it here.\r\n\r\n\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\n\r\n\r\nBuilding a square\r\nThe easiest way to build a square in ggplot2 is probably using geom_rect, however I did not find a way to rotate the geometry, so I decided to go for a polygon (geom_polygon) and control the position of the 4 vertices outside ggplot. For that I created the following function:\r\n\r\n\r\nsquare<-function(x0=1,y0=1, size=1, angle=0){\r\n  xor<-x0+size/2 #X origin (center of the square)\r\n  yor<-y0+size/2 #Y origin (center of the square)\r\n  \r\n  tibble(\r\n    x=c(x0,x0+size,x0+size,x0),\r\n    y=c(y0,y0,y0+size,y0+size)\r\n  )%>%mutate(x2=(x-xor)*cos(angle)-(y-yor)*sin(angle)+xor, #For rotation\r\n             y2=(x-xor)*sin(angle)+(y-yor)*cos(angle)+yor) #for rotation\r\n}\r\n\r\n\r\nWhere x0 and y0 control the position of lower left (or other depending on the axis orientation) vertex, size the length of the sides and angle the amount of rotation from the original position. An x and y for each vertex is set as a template relative to x0, y0 and size. In this way, we will be able to control the position of the square in the canvas. x2 and y2 are set as the rotated coordinates for x and y, respectively. Ultimately, we will use x2 and y2, that will be equal to x and y if angle is 0. I decided to generate new columns for the rotations so I can plot both, the original x, y and the rotated (I confess I had to brush up my trigonometry and google a few things for this).\r\n\r\n\r\nggplot(square())+\r\n  geom_polygon(aes(x=x, y=y))+\r\n  coord_fixed()# just so that axis are equally spaced\r\n\r\n\r\n\r\nWe have our first square!! We can now play with fill and color\r\n\r\n\r\nggplot(square())+\r\n  geom_polygon(aes(x=x, y=y), fill=NA, color='black')+\r\n  coord_fixed()\r\n\r\n\r\n\r\nLet’s play with some parameters\r\n\r\n\r\nggplot(square(size=2,angle=pi/4))+#size 2 and rotate 45 degrees\r\n  geom_polygon(aes(x=x, y=y), fill=NA, color='black')+\r\n  geom_polygon(aes(x=x2, y=y2), fill=NA, color='red')+\r\n  coord_fixed()\r\n\r\n\r\n\r\nHow many squares we want?\r\nSchotter can be seen as a matrix of squares. A simple way to iterate through a matrix is to nest a loop inside a loop such that one loop goes through the rows and another through the columns.\r\nThe original Schotter has 12 columns and 24 rows (that I counted)\r\n\r\n\r\nn<-24*12\r\ndf.list<-list()\r\n  for (j in 1:24){ #iterate through the rows\r\n    for (i in 1:12){ #iterate through the columns\r\n      temp<-square(x=i, y=j) #create a square at column i and row j\r\n      df.list[[n]]<-temp #save the data frame with the square n on a list\r\n      n<-n-1\r\n    }\r\n  }\r\n\r\n\r\nThis chunk creates a data frame containing the x, y, x2 and y2 for 288 (24x12) squares, that we can plotted. To plot each square independently as we will need for later, we need to add a new geom_polygon layer for each square to a ggplot object. We could have done this in the previous loops, but in the course of this mini project I have learned that you can pass a list to ggplot. So, we can iterate though our list of square data frames using lapply and add a geom_polygon for each square (a bit slow on my friend).\r\n\r\n\r\nggplot() + \r\n  lapply(df.list, function(square_data) {\r\n    geom_polygon(data = square_data, aes(x = x2, y = y2), fill=NA, color='black')}\r\n  )+\r\n  coord_fixed()\r\n\r\n\r\n\r\nNow we have the grid of squares. At this point I have decided to deal with the unnecessary elements of the plot. I generated a theme function such that I can easily change the background color.\r\n\r\n\r\ntheme_background<-function(color='white'){\r\n  theme(axis.ticks = element_blank(), axis.text = element_blank(),\r\n        panel.background = element_blank(), panel.grid = element_blank(),\r\n        plot.background = element_rect(fill = color),\r\n        strip.background = element_rect(fill=color),strip.text = element_blank(),\r\n        axis.title = element_blank(), legend.position = 'none')\r\n}\r\n\r\nggplot() + \r\n  lapply(df.list, function(square_data) {\r\n    geom_polygon(data = square_data, aes(x = x2, y = y2), fill=NA, color='black')}\r\n  )+\r\n  coord_fixed()+\r\n  theme_background()\r\n\r\n\r\n\r\nGenerative process\r\nGenerative art is define by wikipedia as:\r\nGenerative art refers to art that in whole or in part has been created with the use of an autonomous system. An autonomous system in this context is generally one that is non-human and can independently determine features of an artwork that would otherwise require decisions made directly by the artist. \r\nAn easy way to give autonomy to our code is by introducing randomness. As we can appreciate from Schotter, both the position and the angle of the squares seems to be more random and move away from the ‘calm’ stage as we move through the rows. We will add this to our nested loops by generating a random displacement and rotation for each square that increases by row.\r\n\r\n\r\nn<-24*12\r\ndf.list<-list()\r\n  for (j in 1:24){ #iterate through the rows\r\n    for (i in 1:12){ #iterate through the columns\r\n      displace<-runif(1,-j,j) #a random number is generated from a uniform distribution with min=-j and max=j\r\n      rotate<-runif(1,-j,j) #random number to rotate the square\r\n      temp<-square(x=i+displace, y=j+displace, angle=rotate) #create a square at column i and row j displaced by displace\r\n      df.list[[n]]<-temp #save the data frame with the square n on a list\r\n      n<-n-1\r\n    }\r\n  }\r\n\r\nggplot() + \r\n  lapply(df.list, function(square_data) {\r\n    geom_polygon(data = square_data, aes(x = x2, y = y2), fill=NA, color='black')}\r\n  )+\r\n  coord_fixed()+\r\n  theme_background()\r\n\r\n\r\n\r\nUps! Something isn’t quite right (although I like it!). I think is because j, that controls how much displacement and rotation can be added get to be too big in respect to x and y. So, let’s control for that. For that I divided j by 40 in the case of displace and by 100 in the case of rotate. I found these numbers by playing around. I also flipped y axis.\r\n\r\n\r\nn<-24*12\r\ndf.list<-list()\r\n  for (j in 1:24){ #iterate through the rows\r\n    for (i in 1:12){ #iterate through the columns\r\n      displace<-runif(1,-j/40,j/40) #a random number is generated from a uniform distribution with min=-j and max=j\r\n      rotate<-runif(1,-j/100,j/100) #random number to rotate the square\r\n      temp<-square(x=i+displace, y=j+displace, angle=rotate) #create a square at column i and row j displaced by displace\r\n      df.list[[n]]<-temp #save the data frame with the square n on a list\r\n      n<-n-1\r\n    }\r\n  }\r\n\r\nggplot() + \r\n  lapply(df.list, function(square_data) {\r\n    geom_polygon(data = square_data, aes(x = x2, y = y2), fill=NA, color='black')}\r\n  )+\r\n  coord_fixed()+\r\n  theme_background()+\r\n  scale_y_reverse()\r\n\r\n\r\n\r\nAnd voila! we have coded a generative algorithm to produce something like Schotter. Because the randomness, every time we run the code, the result will be different.\r\nNow we wrap the code in a function so that we can easily change parameters\r\n\r\n\r\nSchotter<-function(ncol_s=12, nrow_s=24, control_dis=40, \r\n                   control_rot=100, fill_s=NA, color_s='black', alpha_s=0.2, \r\n                   back_color='white'){\r\nn<-ncol_s*nrow_s\r\ndf.list<-list()\r\n  for (j in 1:nrow_s){\r\n    for (i in 1:ncol_s){\r\n      displace<-runif(1,-j/control_dis,j/control_dis)\r\n      rotate<-runif(1,-j/control_rot,j/control_rot)\r\n      temp<-square(x=i+displace, y=j+displace, angle=rotate)\r\n      temp$s<-rep(n)\r\n      df.list[[n]]<-temp\r\n      n<-n-1\r\n    }\r\n  }\r\n\r\nggplot() + \r\n  lapply(df.list, function(square_data) {\r\n    geom_polygon(data = square_data, aes(x = x2, y = y2),\r\n                 alpha=alpha_s, fill=fill_s, color=color_s)}\r\n  )+\r\n  theme_background(color = back_color)+\r\n  coord_fixed()+\r\n  scale_y_reverse()\r\n}  \r\n\r\n\r\nWe can now play!\r\n\r\n\r\nSchotter()\r\n\r\n\r\n\r\n\r\n\r\nSchotter(fill='firebrick', color='orange', nrow_s = 48, control_dis = 20)\r\n\r\n\r\n\r\nThe End\r\nThis is all for now. There are a few ways the code could be improved, but I am happy with the result and I have had fun and learned something new!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-06-18-reproducing-schotter-in-r/reproducing-schotter-in-r_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2024-12-16T17:13:37-05:00",
    "input_file": "reproducing-schotter-in-r.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-01-14-from-wet-lab-to-data-science/",
    "title": "From wet lab to data science",
    "description": "How I think experimental researchers are data scientist",
    "author": [
      {
        "name": "Abel Torres Espín",
        "url": {}
      }
    ],
    "date": "2022-01-14",
    "categories": [
      "Data science"
    ],
    "contents": "\r\nNot while ago, I introduced myself as a data scientist, but it has not always been like that. Let me tell you how a biologist like me gets to that and how I believe any researcher can. I am a biomedical researcher and I spent the first 10 years of my career performing lab experiments, what is known as “wet lab”. At some point I realized that doing experiments was not that exciting anymore (a story for another day), but I was not ready to stop doing research, so I decided to shift focus to a more data-intensive work. For the last 3 years I have moved from a pure wet lab work to a pure dry or computational professional activity. During this time, I have learned a lot of new stuff, but I have also got to the conclusion that researchers have a good head start to transition to data science and related work. After all, data is the prima matter for contemporary research. Don’t make me wrong, is not an easy path, but if you are reading this because you are thinking about it, let me convince you that your research knowledge and skills are not that far from those needed to start a career in data science.\r\n\r\nNOTE: If you are a computational researcher or your work is heavy on the quantitative site, you probably already made the connections I am making here and you probably already call yourself a data scientist. I have the perception that there is an increase in the number of researchers that has some working experience on computational methods, but yet are not fully aware of it.\r\n\r\nWe scientist have a lot of work qualities, but acknowledging our transferable skills is not one of them. So let me illustrate my point. First thing first, researchers like to ask questions about the unknown and generate hypothesis about how they think things work. And how do we answer those questions? We turn into data. No matter your field, if you are doing research today, you probably have had to deal with data. What data scientist do? They ask questions, using data to answer them. Not that different so far. Let’s dissect this process and see how researcher’s skills can translate to data science.\r\nAsk your question: We went over this already, all starts with some questions, whether you do research or business analytics, you always want to know something new and researchers sick the limits of knowledge by definition.\r\nGather your data: Here researchers excel! After years of research work, I am pretty sure you know your ups and downs about doing literature review, designing an experiment tailored to answer your question, and painstikly performed experiment/s and data collection. Data scientist do the same but using other tools. Actually, depending on their work, data scientist might not do any data gathering themselves. Researchers might also get the data already collected, but if you have done research for a while, you would probably be asking a lot of questions on how the data came to be.\r\nBuild your dataset: Do you remember all those hours entering numbers in a spreadsheet? You are halfway there then. In my experience, here is where researchers might lack basic concepts such as data structures, relational databases, metadata or data dictionary. So next time you enter data in any sort of digital data keeper, you could start by tidying your dataset (Data organization in spreadsheets) and build a data dictionary for it.\r\nData wrangling: This is a jargon to evoke any process related to joint, transform, re-organize, summarize and re-format datasets. While well formatted data for storage is important, almost every time you want to perform a specific analysis, you will have to make changes in the data structure. Most researchers probably know their way through spreadsheets or some stats software to bring the data to the required format for analysis.\r\nExplore data: Meeting after meeting you show summary statistics and graphs to lab mattes, supervisors and collaborators. That is really it, you already know how to do this. Produce some summaries and visuals to start getting a sense of the data, check for potential errors and trends, etc.\r\nData analysis: If you have performed and analyzed research experiments, you probably navigated things like descriptive statistics, hypothesis testing, t-test, analysis of variance (ANOVA), p-values, parametric and non-parametric test, linear regression, correlation, normal distribution, to say a few. So you know more analysis and statistics that you might think. Although data analysis can be very complex and sophisticated in data science, researchers are very well equipped to do a lot of analytical task.\r\nGet insights: Interpret your results. As a researcher, I believe this is one of your powers.\r\nCommunication, data visualization and storytelling: If anyone things that researchers are these robots that communicate between them using dull technical reports full of numbers and lacking literary elements, will be very surprised at reading a scientific manuscript. Yes, these documents have more jargon and terminology than the average read, but nowadays manuscript and grant writing is an art of effective communication. How to say as much as possible, in the minimum space, for a broad audience (editors, reviewers and a broad reach of researchers), and at the same time captivate and avoid boresome. You know how to sell your insights. An although I think there is still a lot of space for improvement in scientific communication using viz, you know how to make pretty tables, plots, figures, posters and slide presentations.\r\nLearn new tools: there are a lot of tools in data science that researchers don’t know about. And the number keeps increasing at a scary paste. But so it is in research. Both researchers and data scientist are used to the need of keeping up with new methods and learning some of them in detail. Don’t let this scare you.\r\nAs you can see, researchers and data scientist share more than what you might think at first glance. Sure you will have to learn a lot, and sure will get frustrating some times, but if research has taught me something is how to constantly learn and how to endure these frustrations and transform them into growth. If you are a researcher that have considered the idea of changing to a more data-centric role, I hope I have convinced you that you are not starting from nothing and I hope I can ease a bit the anxiety of confronting a change like this.\r\n\r\nRECOMMENDATION: I truly believe researchers have a lot of transferable skills to data science. If I might give a recommendation for those wanting to do the transition, I would say “learn the terminology and start learning some coding”. Once you know how things are called in data science, you will realize you know more than what you think you know. Code will make your life easier even if you decide to stay in research after all.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-12-16T17:12:29-05:00",
    "input_file": "from-wet-lab-to-data-science.knit.md"
  }
]
